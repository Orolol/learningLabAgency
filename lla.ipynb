{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/orolol/workspace\n"
     ]
    }
   ],
   "source": [
    "!pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>full_text</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000d118</td>\n",
       "      <td>Many people have car where they live. The thin...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fe60</td>\n",
       "      <td>I am a scientist at NASA that is discussing th...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001ab80</td>\n",
       "      <td>People always wish they had the same technolog...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001bdc0</td>\n",
       "      <td>We all heard about Venus, the planet without a...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>002ba53</td>\n",
       "      <td>Dear, State Senator\\n\\nThis is a letter to arg...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  essay_id                                          full_text  score\n",
       "0  000d118  Many people have car where they live. The thin...      3\n",
       "1  000fe60  I am a scientist at NASA that is discussing th...      3\n",
       "2  001ab80  People always wish they had the same technolog...      4\n",
       "3  001bdc0  We all heard about Venus, the planet without a...      4\n",
       "4  002ba53  Dear, State Senator\\n\\nThis is a letter to arg...      3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('kaggle-compet/learningLabAgency/data/train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, features, ffn_features, n_heads,\n",
    "        rezero=True, **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(features)\n",
    "        self.atten = nn.MultiheadAttention(features, n_heads)\n",
    "\n",
    "        self.norm2 = nn.LayerNorm(features)\n",
    "        self.ffn = PositionWise(features, ffn_features)\n",
    "\n",
    "        self.rezero = rezero\n",
    "\n",
    "        if rezero:\n",
    "            self.re_alpha = nn.Parameter(torch.zeros((1, )))\n",
    "        else:\n",
    "            self.re_alpha = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Step 1: Multi-Head Self Attention\n",
    "        y1 = self.norm1(x)\n",
    "        y1, _atten_weights = self.atten(y1, y1, y1)\n",
    "\n",
    "        y = x + self.re_alpha * y1\n",
    "\n",
    "        # Step 2: PositionWise Feed Forward Network\n",
    "        y2 = self.norm2(y)\n",
    "        y2 = self.ffn(y2)\n",
    "\n",
    "        y = y + self.re_alpha * y2\n",
    "\n",
    "        return y\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return 're_alpha = %e' % (self.re_alpha, )\n",
    "\n",
    "\n",
    "class PositionWise(nn.Module):\n",
    "\n",
    "    def __init__(self, features, ffn_features, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(features, ffn_features),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(ffn_features, features),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, features, ffn_features, n_heads, n_blocks,\n",
    "        rezero=True, **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.encoder = nn.Sequential(*[\n",
    "            TransformerBlock(\n",
    "                features, ffn_features, n_heads, rezero\n",
    "            ) for _ in range(n_blocks)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x.size())\n",
    "        #y = x.permute((1, 0, 2))\n",
    "        y = self.encoder(x)\n",
    "\n",
    "        # result : (N, L, features)\n",
    "        result = y.permute((1, 0, 2))\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoModel\n",
    "\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, transformer_model_name, num_labels):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        self.transformer = AutoModel.from_pretrained(transformer_model_name)\n",
    "        self.classifier = nn.Linear(self.transformer.config.hidden_size, num_labels)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Pass the inputs through the transformer model\n",
    "        outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # We take only the outputs from the first token (CLS token) for classification\n",
    "        cls_output = outputs[0][:, 0, :]\n",
    "        \n",
    "        # Pass the CLS token outputs through the classifier to get predictions\n",
    "        logits = self.classifier(cls_output)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Example usage\n",
    "model_name = \"bert-base-uncased\"  # You can replace this with any model from Hugging Face's model hub\n",
    "num_labels = 6  # For binary classification, adjust according to your task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['many', 'people', 'have', 'car', 'where', 'they', 'live', '.', 'the', 'thing', 'they', 'don', \"'\", 't', 'know', 'is', 'that', 'when', 'you', 'use', 'a', 'car', 'al', '##ot', 'of', 'thing', 'can', 'happen', 'like', 'you', 'can', 'get', 'in', 'acc', '##ide', '##t', 'or', 'the', 'smoke', 'that', 'the', 'car', 'has', 'is', 'bad', 'to', 'breath', 'on', 'if', 'someone', 'is', 'walk', 'but', 'in', 'va', '##uba', '##n', ',', 'germany', 'they', 'don', '##t', 'have', 'that', 'pro', '##ble', 'because', '70', 'percent', 'of', 'va', '##uba', '##n', \"'\", 's', 'families', 'do', 'not', 'own', 'cars', ',', 'and', '57', 'percent', 'sold', 'a', 'car', 'to', 'move', 'there', '.', 'street', 'park', '##ig', ',', 'driveway', '##s', 'and', 'home', 'garage', '##s', 'are', 'forbidden', 'on', 'the', 'outskirts', 'of', 'fr', '##ei', '##bu', '##rd', 'that', 'near', 'the', 'french', 'and', 'swiss', 'borders', '.', 'you', 'pro', '##bal', '##y', 'won', \"'\", 't', 'see', 'a', 'car', 'in', 'va', '##uba', '##n', \"'\", 's', 'streets', 'because', 'they', 'are', 'completely', '\"', 'car', 'free', '\"', 'but', 'if', 'some', 'that', 'lives', 'in', 'va', '##uba', '##n', 'that', 'owns', 'a', 'car', 'ownership', 'is', 'allowed', ',', 'but', 'there', 'are', 'only', 'two', 'places', 'that', 'you', 'can', 'park', 'a', 'large', 'garage', '##s', 'at', 'the', 'edge', 'of', 'the', 'development', ',', 'where', 'a', 'car', 'owner', 'buys', 'a', 'space', 'but', 'it', 'not', 'cheap', 'to', 'buy', 'one', 'they', 'sell', 'the', 'space', 'for', 'you', 'car', 'for', '$', '40', ',', '000', 'along', 'with', 'a', 'home', '.', 'the', 'va', '##uba', '##n', 'people', 'completed', 'this', 'in', '2006', ',', 'they', 'said', 'that', 'this', 'an', 'example', 'of', 'a', 'growing', 'trend', 'in', 'europe', ',', 'the', 'until', '##e', 'states', 'and', 'some', 'where', 'else', 'are', 'suburban', 'life', 'from', 'auto', 'use', 'this', 'is', 'called', '\"', 'smart', 'planning', '\"', '.', 'the', 'current', 'efforts', 'to', 'drastically', 'reduce', 'greenhouse', 'gas', 'emissions', 'from', 'tail', '##es', 'the', 'pass', '##eng', '##ee', 'cars', 'are', 'responsible', 'for', '12', 'percent', 'of', 'greenhouse', 'gas', 'emissions', 'in', 'europe', 'and', 'up', 'to', '50', 'percent', 'in', 'some', 'car', 'intensive', 'in', 'the', 'united', 'states', '.', 'i', 'hon', '##es', '##lty', 'think', 'that', 'good', 'idea', 'that', 'they', 'did', 'that', 'is', 'va', '##uda', '##n', 'because', 'that', 'makes', 'cities', 'dense', '##r', 'and', 'better', 'for', 'walking', 'and', 'in', 'va', '##uba', '##n', 'there', 'are', '5', ',', '500', 'residents', 'within', 'a', 'rectangular', 'square', 'mile', '.', 'in', 'the', 'art', '##ical', 'david', 'gold', 'berg', 'said', 'that', '\"', 'all', 'of', 'our', 'development', 'since', 'world', 'war', '2', 'has', 'been', 'centered', 'on', 'the', 'cars', ',', 'and', 'that', 'will', 'have', 'to', 'change', '\"', 'and', 'i', 'think', 'that', 'was', 'very', 'true', 'what', 'david', 'gold', 'said', 'because', 'al', '##ot', 'thing', 'we', 'need', 'cars', 'to', 'do', 'we', 'can', 'go', 'anyway', 'were', 'with', 'out', 'cars', 'bea', '##cus', '##e', 'some', 'people', 'are', 'a', 'very', 'lazy', 'to', 'walk', 'to', 'place', 'that', '##s', 'why', 'they', 'al', '##ot', 'of', 'people', 'use', 'car', 'and', 'i', 'think', 'that', 'it', 'was', 'a', 'good', 'idea', 'that', 'that', 'they', 'did', 'that', 'in', 'va', '##uba', '##n', 'so', 'people', 'can', 'see', 'how', 'we', 'really', 'don', \"'\", 't', 'need', 'car', 'to', 'go', 'to', 'place', 'from', 'place', 'because', 'we', 'can', 'walk', 'from', 'were', 'we', 'need', 'to', 'go', 'or', 'we', 'can', 'ride', 'by', '##cles', 'with', 'out', 'the', 'use', 'of', 'a', 'car', '.', 'it', 'good', 'that', 'they', 'are', 'doing', 'that', 'if', 'you', 'th', '##ik', 'about', 'your', 'help', 'the', 'earth', 'in', 'way', 'and', 'that', '##s', 'a', 'very', 'good', 'thing', 'to', '.', 'in', 'the', 'united', 'states', ',', 'the', 'environmental', 'protection', 'agency', 'is', 'promoting', 'what', 'is', 'called', '\"', 'car', 'reduced', '\"', 'com', '##mun', '##tu', '##nti', '##es', ',', 'and', 'the', 'legislators', 'are', 'starting', 'to', 'act', ',', 'if', 'cautiously', '.', 'ma', '##any', 'experts', 'expect', 'pub', '##ic', 'transport', 'serving', 'suburbs', 'to', 'play', 'a', 'much', 'larger', 'role', 'in', 'a', 'new', 'six', 'years', 'federal', 'transportation', 'bill', 'to', 'approved', 'this', 'year', '.', 'in', 'previous', 'bill', ',', '80', 'percent', 'of', 'appropriations', 'have', 'by', 'law', 'gone', 'to', 'highways', 'and', 'only', '20', 'percent', 'to', 'other', 'transports', '.', 'there', 'many', 'good', 'reason', 'why', 'they', 'should', 'do', 'this', '.']\n",
      "Length: 608\n"
     ]
    }
   ],
   "source": [
    "# DF is comprised of 3 columns: 'essay_id', 'full_text', 'score'\n",
    "# 'essay_id' is a unique identifier for each essay\n",
    "# 'full_text' is the text of the essay\n",
    "# 'score' is the score of the essay\n",
    "# 'score' is the target variable\n",
    "\n",
    "# We will use 'full_text' as the input and 'score' as the target variable\n",
    "# First let's encode the 'full_text' using BERT\n",
    "\n",
    "# We will use the 'transformers' library by Hugging Face\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# First let's tokenize the 'full_text'\n",
    "text = df['full_text'][0]\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print('Tokens:', tokens)\n",
    "print('Length:', len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [2116, 2111, 2031, 2482, 2073, 2027, 2444, 1012, 1996, 2518, 2027, 2123, 1005, 1056, 2113, 2003, 2008, 2043, 2017, 2224, 1037, 2482, 2632, 4140, 1997, 2518, 2064, 4148, 2066, 2017, 2064, 2131, 1999, 16222, 5178, 2102, 2030, 1996, 5610, 2008, 1996, 2482, 2038, 2003, 2919, 2000, 3052, 2006, 2065, 2619, 2003, 3328, 2021, 1999, 12436, 19761, 2078, 1010, 2762, 2027, 2123, 2102, 2031, 2008, 4013, 3468, 2138, 3963, 3867, 1997, 12436, 19761, 2078, 1005, 1055, 2945, 2079, 2025, 2219, 3765, 1010, 1998, 5401, 3867, 2853, 1037, 2482, 2000, 2693, 2045, 1012, 2395, 2380, 8004, 1010, 11202, 2015, 1998, 2188, 7381, 2015, 2024, 10386, 2006, 1996, 12730, 1997, 10424, 7416, 8569, 4103, 2008, 2379, 1996, 2413, 1998, 5364, 6645, 1012, 2017, 4013, 10264, 2100, 2180, 1005, 1056, 2156, 1037, 2482, 1999, 12436, 19761, 2078, 1005, 1055, 4534, 2138, 2027, 2024, 3294, 1000, 2482, 2489, 1000, 2021, 2065, 2070, 2008, 3268, 1999, 12436, 19761, 2078, 2008, 8617, 1037, 2482, 6095, 2003, 3039, 1010, 2021, 2045, 2024, 2069, 2048, 3182, 2008, 2017, 2064, 2380, 1037, 2312, 7381, 2015, 2012, 1996, 3341, 1997, 1996, 2458, 1010, 2073, 1037, 2482, 3954, 23311, 1037, 2686, 2021, 2009, 2025, 10036, 2000, 4965, 2028, 2027, 5271, 1996, 2686, 2005, 2017, 2482, 2005, 1002, 2871, 1010, 2199, 2247, 2007, 1037, 2188, 1012, 1996, 12436, 19761, 2078, 2111, 2949, 2023, 1999, 2294, 1010, 2027, 2056, 2008, 2023, 2019, 2742, 1997, 1037, 3652, 9874, 1999, 2885, 1010, 1996, 2127, 2063, 2163, 1998, 2070, 2073, 2842, 2024, 9282, 2166, 2013, 8285, 2224, 2023, 2003, 2170, 1000, 6047, 4041, 1000, 1012, 1996, 2783, 4073, 2000, 21040, 5547, 16635, 3806, 11768, 2013, 5725, 2229, 1996, 3413, 13159, 4402, 3765, 2024, 3625, 2005, 2260, 3867, 1997, 16635, 3806, 11768, 1999, 2885, 1998, 2039, 2000, 2753, 3867, 1999, 2070, 2482, 11806, 1999, 1996, 2142, 2163, 1012, 1045, 10189, 2229, 24228, 2228, 2008, 2204, 2801, 2008, 2027, 2106, 2008, 2003, 12436, 14066, 2078, 2138, 2008, 3084, 3655, 9742, 2099, 1998, 2488, 2005, 3788, 1998, 1999, 12436, 19761, 2078, 2045, 2024, 1019, 1010, 3156, 3901, 2306, 1037, 10806, 2675, 3542, 1012, 1999, 1996, 2396, 7476, 2585, 2751, 15214, 2056, 2008, 1000, 2035, 1997, 2256, 2458, 2144, 2088, 2162, 1016, 2038, 2042, 8857, 2006, 1996, 3765, 1010, 1998, 2008, 2097, 2031, 2000, 2689, 1000, 1998, 1045, 2228, 2008, 2001, 2200, 2995, 2054, 2585, 2751, 2056, 2138, 2632, 4140, 2518, 2057, 2342, 3765, 2000, 2079, 2057, 2064, 2175, 4312, 2020, 2007, 2041, 3765, 26892, 7874, 2063, 2070, 2111, 2024, 1037, 2200, 13971, 2000, 3328, 2000, 2173, 2008, 2015, 2339, 2027, 2632, 4140, 1997, 2111, 2224, 2482, 1998, 1045, 2228, 2008, 2009, 2001, 1037, 2204, 2801, 2008, 2008, 2027, 2106, 2008, 1999, 12436, 19761, 2078, 2061, 2111, 2064, 2156, 2129, 2057, 2428, 2123, 1005, 1056, 2342, 2482, 2000, 2175, 2000, 2173, 2013, 2173, 2138, 2057, 2064, 3328, 2013, 2020, 2057, 2342, 2000, 2175, 2030, 2057, 2064, 4536, 2011, 18954, 2007, 2041, 1996, 2224, 1997, 1037, 2482, 1012, 2009, 2204, 2008, 2027, 2024, 2725, 2008, 2065, 2017, 16215, 5480, 2055, 2115, 2393, 1996, 3011, 1999, 2126, 1998, 2008, 2015, 1037, 2200, 2204, 2518, 2000]\n",
      "Input IDs shape: torch.Size([1, 512])\n",
      "Last hidden states shape: torch.Size([1, 512, 768])\n"
     ]
    }
   ],
   "source": [
    "# Now we need to embed the tokens using BERT\n",
    "\n",
    "# First let's convert the tokens to token IDs\n",
    "\n",
    "# Truncate the tokens to fit the BERT's maximum sequence length\n",
    "if len(tokens) > 512:\n",
    "    tokens = tokens[:512]\n",
    "\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print('Token IDs:', token_ids)\n",
    "\n",
    "# Now let's convert the token IDs to embeddings\n",
    "\n",
    "input_ids = torch.tensor(token_ids).unsqueeze(0)\n",
    "print('Input IDs shape:', input_ids.shape)\n",
    "outputs = model(input_ids)\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "print('Last hidden states shape:', last_hidden_states.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's create a custom PyTorch Dataset\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class EssayDataset(Dataset):\n",
    "    \n",
    "        def __init__(self, df, tokenizer, model):\n",
    "            self.df = df\n",
    "            self.tokenizer = tokenizer\n",
    "            self.model = model\n",
    "    \n",
    "        def __len__(self):\n",
    "            return len(self.df)\n",
    "    \n",
    "        def __getitem__(self, idx):\n",
    "            text = self.df['full_text'][idx]\n",
    "            tokens = self.tokenizer.tokenize(text)\n",
    "    \n",
    "            if len(tokens) > 512:\n",
    "                tokens = tokens[:512]\n",
    "                \n",
    "            token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "            \n",
    "            #Let's padd the token_ids\n",
    "            token_ids = token_ids + [0] * (512 - len(token_ids))\n",
    "            \n",
    "            input_ids = torch.tensor(token_ids).unsqueeze(0)\n",
    "    \n",
    "            outputs = self.model(input_ids)\n",
    "            last_hidden_states = outputs.last_hidden_state\n",
    "    \n",
    "            return last_hidden_states, self.df['score'][idx]\n",
    "        \n",
    "dataset = EssayDataset(df, tokenizer, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now let's create a custom PyTorch DataLoader\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Now let's create a simple Transformer model\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        self.encoder = TransformerEncoder(\n",
    "            features=768,\n",
    "            ffn_features=3072,\n",
    "            n_heads=12,\n",
    "            n_blocks=12\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(768, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "model = TransformerClassifier(model_name, num_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strat training\n",
      "Epoch 1, Batch 1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TransformerClassifier.forward() missing 1 required positional argument: 'attention_mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 18\u001b[0m\n\u001b[1;32m     14\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 18\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     20\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.0/envs/orolol/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.0/envs/orolol/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: TransformerClassifier.forward() missing 1 required positional argument: 'attention_mask'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Now let's train the model\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Strat training\")\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        print(f\"Epoch {epoch + 1}, Batch {i + 1}\")\n",
    "        inputs, labels = data\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % 100 == 99:\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "            \n",
    "print('Finished Training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "orolol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
